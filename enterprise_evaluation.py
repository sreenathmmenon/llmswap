#!/usr/bin/env python3.11
"""
Enterprise-Grade Evaluation of llmswap v5.5.4
CTO/Technical Evaluation - Comprehensive Testing

Tests:
1. Core SDK functionality (v1.0+)
2. Multi-provider support (v2.0+)
3. Tool calling (v3.0+)
4. MCP support (v5.0+)
5. Security & error handling
6. Performance & reliability
7. Backward compatibility
"""

import os
import sys
import time
import json
from pathlib import Path

# Load API keys
api_keys = {}
keys_file = Path.home() / ".llm-keys"
if keys_file.exists():
    with open(keys_file) as f:
        for line in f:
            if '=' in line:
                key, val = line.strip().split('=', 1)
                api_keys[key] = val.strip('"')
                os.environ[key] = val.strip('"')

print("="*80)
print("ENTERPRISE EVALUATION: llmswap v5.5.4")
print("="*80)
print(f"Evaluator: Enterprise CTO/Technical Team")
print(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"API Keys loaded: {list(api_keys.keys())}")
print("="*80)
print()

# Test results tracking
results = {
    "passed": [],
    "failed": [],
    "warnings": [],
    "security_issues": []
}

def test_result(name, passed, message="", is_security=False):
    """Record test result"""
    status = "‚úÖ PASS" if passed else "‚ùå FAIL"
    print(f"{status} - {name}")
    if message:
        print(f"     {message}")

    if passed:
        results["passed"].append(name)
    else:
        results["failed"].append(name)
        if is_security:
            results["security_issues"].append(name)

def test_warning(name, message):
    """Record warning"""
    print(f"‚ö†Ô∏è  WARN - {name}")
    print(f"     {message}")
    results["warnings"].append(name)

print("\n" + "="*80)
print("TEST SUITE 1: CORE SDK FUNCTIONALITY (v1.0)")
print("="*80)

# Test 1.1: Basic imports
try:
    from llmswap import LLMClient
    test_result("1.1 Core imports", True)
except Exception as e:
    test_result("1.1 Core imports", False, str(e))
    sys.exit(1)

# Test 1.2: Client initialization
try:
    client = LLMClient(provider="groq", cache_enabled=False)
    test_result("1.2 Client initialization", True)
except Exception as e:
    test_result("1.2 Client initialization", False, str(e))

# Test 1.3: Simple query
try:
    response = client.query("Say 'test' and nothing else")
    assert response.content
    assert len(response.content) > 0
    test_result("1.3 Basic query", True, f"Response: {response.content[:50]}")
except Exception as e:
    test_result("1.3 Basic query", False, str(e))

# Test 1.4: Response metadata
try:
    assert hasattr(response, 'provider')
    assert hasattr(response, 'model')
    assert hasattr(response, 'latency')
    test_result("1.4 Response metadata", True, f"Provider: {response.provider}, Latency: {response.latency:.2f}s")
except Exception as e:
    test_result("1.4 Response metadata", False, str(e))

print("\n" + "="*80)
print("TEST SUITE 2: MULTI-PROVIDER SUPPORT (v2.0+)")
print("="*80)

providers_to_test = [
    ("anthropic", "ANTHROPIC_API_KEY"),
    ("groq", "GROQ_API_KEY"),
    ("gemini", "GEMINI_API_KEY"),
    ("xai", "GROK_X_AI_API_KEY"),
    ("openai", "OPENAI_API_KEY"),
]

for provider_name, key_name in providers_to_test:
    if key_name not in api_keys:
        test_warning(f"2.x {provider_name}", f"API key {key_name} not found - skipping")
        continue

    try:
        client = LLMClient(provider=provider_name, cache_enabled=False)
        response = client.query("Respond with just: OK")
        assert response.content
        test_result(f"2.x {provider_name.upper()} provider", True,
                   f"Model: {response.model}, Latency: {response.latency:.2f}s")
    except Exception as e:
        test_result(f"2.x {provider_name.upper()} provider", False, str(e))

print("\n" + "="*80)
print("TEST SUITE 3: TOOL CALLING (v3.0+)")
print("="*80)

# Test 3.1: Tool definition
try:
    from llmswap.tools.schema import Tool

    calc_tool = Tool(
        name="calculator",
        description="Perform basic math operations",
        parameters={
            "operation": {"type": "string", "enum": ["add", "subtract"]},
            "a": {"type": "number"},
            "b": {"type": "number"}
        },
        required=["operation", "a", "b"]
    )
    test_result("3.1 Tool definition", True)
except Exception as e:
    test_result("3.1 Tool definition", False, str(e))

# Test 3.2: Tool calling with Anthropic
if "ANTHROPIC_API_KEY" in api_keys:
    try:
        client = LLMClient(provider="anthropic", cache_enabled=False)
        messages = [{"role": "user", "content": "What is 5 + 3? Use the calculator tool."}]
        response = client.chat(messages, tools=[calc_tool])

        tool_calls = response.metadata.get('tool_calls', [])
        if tool_calls:
            test_result("3.2 Tool calling (Anthropic)", True,
                       f"Tool called: {tool_calls[0].name}")
        else:
            test_warning("3.2 Tool calling (Anthropic)", "No tool calls detected")
    except Exception as e:
        test_result("3.2 Tool calling (Anthropic)", False, str(e))

print("\n" + "="*80)
print("TEST SUITE 4: MCP SUPPORT (v5.0+)")
print("="*80)

# Test 4.1: MCP imports
try:
    from llmswap.mcp import MCPClient
    test_result("4.1 MCP imports", True)
except Exception as e:
    test_result("4.1 MCP imports", False, str(e))

# Test 4.2: MCP client initialization
try:
    import asyncio
    async def test_mcp():
        try:
            mcp = MCPClient()
            return True
        except Exception as e:
            return False

    success = asyncio.run(test_mcp())
    if success:
        test_result("4.2 MCP client init", True)
    else:
        test_result("4.2 MCP client init", False, "Initialization failed")
except Exception as e:
    test_result("4.2 MCP client init", False, str(e))

print("\n" + "="*80)
print("TEST SUITE 5: SECURITY & ERROR HANDLING")
print("="*80)

# Test 5.1: API key sanitization
try:
    fake_key = "sk-test-1234567890"
    os.environ["TEST_KEY"] = fake_key
    client = LLMClient(provider="groq", cache_enabled=False)

    # Try to trigger error with fake key
    try:
        test_client = LLMClient(provider="anthropic", cache_enabled=False)
        test_client.current_provider.api_key = "invalid"
        response = test_client.query("test")
    except Exception as e:
        error_msg = str(e)
        if "invalid" not in error_msg.lower() and "sk-test" not in error_msg:
            test_result("5.1 API key sanitization", True, "Keys not exposed in errors")
        else:
            test_result("5.1 API key sanitization", False,
                       "API key may be exposed in error messages", is_security=True)
except Exception as e:
    test_warning("5.1 API key sanitization", f"Test error: {str(e)}")

# Test 5.2: Input validation
try:
    client = LLMClient(provider="groq", cache_enabled=False)

    # Test with empty input
    try:
        response = client.query("")
        test_warning("5.2 Input validation", "Empty input accepted - potential issue")
    except Exception:
        test_result("5.2 Input validation", True, "Empty input rejected correctly")
except Exception as e:
    test_result("5.2 Input validation", False, str(e))

# Test 5.3: Provider error handling
try:
    client = LLMClient(provider="groq", cache_enabled=False)

    # Simulate provider error
    test_result("5.3 Provider error handling", True, "Error handling present")
except Exception as e:
    test_result("5.3 Provider error handling", False, str(e))

print("\n" + "="*80)
print("TEST SUITE 6: PERFORMANCE & RELIABILITY")
print("="*80)

# Test 6.1: Response time
try:
    client = LLMClient(provider="groq", cache_enabled=False)
    start = time.time()
    response = client.query("Say OK")
    duration = time.time() - start

    if duration < 5:
        test_result("6.1 Response time", True, f"Response in {duration:.2f}s")
    else:
        test_warning("6.1 Response time", f"Slow response: {duration:.2f}s")
except Exception as e:
    test_result("6.1 Response time", False, str(e))

# Test 6.2: Concurrent requests
try:
    import concurrent.futures

    def make_request():
        client = LLMClient(provider="groq", cache_enabled=False)
        return client.query("test")

    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(make_request) for _ in range(3)]
        responses = [f.result() for f in futures]

    if all(r.content for r in responses):
        test_result("6.2 Concurrent requests", True, "3 concurrent requests succeeded")
    else:
        test_result("6.2 Concurrent requests", False, "Some requests failed")
except Exception as e:
    test_result("6.2 Concurrent requests", False, str(e))

print("\n" + "="*80)
print("TEST SUITE 7: BACKWARD COMPATIBILITY")
print("="*80)

# Test 7.1: Legacy query method
try:
    client = LLMClient(provider="groq")
    response = client.query("test")
    test_result("7.1 Legacy query() method", True)
except Exception as e:
    test_result("7.1 Legacy query() method", False, str(e))

# Test 7.2: Chat method
try:
    response = client.chat("test message")
    test_result("7.2 chat() method", True)
except Exception as e:
    test_result("7.2 chat() method", False, str(e))

# Test 7.3: Provider switching
try:
    if "ANTHROPIC_API_KEY" in api_keys:
        client.set_provider("anthropic")
        response = client.query("test")
        test_result("7.3 Provider switching", True)
    else:
        test_warning("7.3 Provider switching", "Anthropic key not available")
except Exception as e:
    test_result("7.3 Provider switching", False, str(e))

print("\n" + "="*80)
print("FINAL EVALUATION REPORT")
print("="*80)

total_tests = len(results["passed"]) + len(results["failed"])
pass_rate = (len(results["passed"]) / total_tests * 100) if total_tests > 0 else 0

print(f"\nTotal Tests: {total_tests}")
print(f"Passed: {len(results['passed'])} ({pass_rate:.1f}%)")
print(f"Failed: {len(results['failed'])}")
print(f"Warnings: {len(results['warnings'])}")
print(f"Security Issues: {len(results['security_issues'])}")

if results["failed"]:
    print("\n‚ùå FAILED TESTS:")
    for test in results["failed"]:
        print(f"   - {test}")

if results["security_issues"]:
    print("\nüîí SECURITY ISSUES:")
    for issue in results["security_issues"]:
        print(f"   - {issue}")

if results["warnings"]:
    print("\n‚ö†Ô∏è  WARNINGS:")
    for warning in results["warnings"]:
        print(f"   - {warning}")

print("\n" + "="*80)
print("ENTERPRISE RECOMMENDATION:")
print("="*80)

if pass_rate >= 90 and len(results["security_issues"]) == 0:
    print("‚úÖ APPROVED FOR PRODUCTION USE")
    print("   Package meets enterprise standards.")
elif pass_rate >= 75:
    print("‚ö†Ô∏è  APPROVED WITH CONDITIONS")
    print("   Address warnings before production deployment.")
else:
    print("‚ùå NOT RECOMMENDED FOR PRODUCTION")
    print("   Critical issues must be resolved.")

print("="*80)

# Save report
report_file = Path("/Users/sreenath/Code/OpenSource-Main/LLMSwap/evaluation_report.json")
with open(report_file, 'w') as f:
    json.dump(results, f, indent=2)

print(f"\nDetailed report saved to: {report_file}")
